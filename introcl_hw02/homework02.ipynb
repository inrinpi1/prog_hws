{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "727bd75d-3dfc-4fe7-b4b6-a251628810a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Задача 1 (10 баллов). \n",
    "\n",
    "Попробуем себя в решении задачи определения темы текста. Будем считать, что два текста похожи по теме, если у них больше общих слов (только не предлогов с союзами), чем у других текстов. У нашей программы для определения темы будет несколько готовых текстов (достаточно больших!) с уже известной темой в базе: выберите тексты (и темы) самостоятельно, 5-6 будет достаточно. \n",
    "\n",
    "Что должна делать программа? При запуске вы ей сообщаете название нового файла с текстом, который нужно классифицировать, она его открывает, обрабатывает и сравнивает с текстами в своей базе. С которым из текстов оказалось больше всего общих слов, того и тема! Очевидно, вам понадобится какие-то слова из текстов отбрасывать (подумайте, каким образом это сделать - здесь на самом деле несколько вариантов концепций), а еще лемматизировать или хотя бы использовать стемминг. \n",
    "\n",
    "Когда будете сдавать это задание, пожалуйста, пришлите и файлы с текстами. И имейте в виду, если тексты будут вставлены прямо в код и слишком короткие, я задачу засчитаю только вполовину. \n",
    "\n",
    "Напоминаю, как открываются файлы:\n",
    "\n",
    "```\n",
    "with open('путь к файлу - пишите прямые слеши', 'r', encoding='utf-8') as f:\n",
    "    text = f.read() # все содержимое вашего файла считается в одну длинную строку\n",
    "```\n",
    "\n",
    "Настоятельно советую оформить код хотя бы в функции. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57165fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'История'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = word_tokenize(text)\n",
    "    words = [stemmer.stem(word) for word in words if word not in stopwords]\n",
    "    return set(words)\n",
    "\n",
    "def compare_texts(input_text, base_texts):\n",
    "    common_words_count = {}\n",
    "\n",
    "    for theme, base_text in base_texts.items():\n",
    "        common_words = input_text.intersection(base_text)\n",
    "        common_words_count[theme] = len(common_words)\n",
    "\n",
    "    best_match_theme = max(common_words_count, key=common_words_count.get)\n",
    "    return best_match_theme\n",
    "\n",
    "\n",
    "stopwords = set(stopwords.words('russian'))\n",
    "stemmer = SnowballStemmer('russian')\n",
    "\n",
    "file_paths = {\n",
    "    'Технологии': r'тексты\\technology.txt',\n",
    "    'Спорт': r'тексты\\sports.txt',\n",
    "    'Путешествия': r'тексты\\travel.txt',\n",
    "    'История': r'тексты\\history.txt',\n",
    "    'Кулинария': r'тексты\\cooking.txt'\n",
    "}\n",
    "\n",
    "texts = {theme: read_file(path) for theme, path in file_paths.items()}\n",
    "processed_texts = {theme: preprocess_text(text) for theme, text in texts.items()}\n",
    "\n",
    "input_text = read_file(r'тексты\\input_history.txt')\n",
    "processed_input = preprocess_text(input_text)\n",
    "classified_theme = compare_texts(processed_input, processed_texts)\n",
    "classified_theme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867f7c4b-0934-4f27-a7b0-edf84185bbf6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Задача 2 (10 баллов). \n",
    "\n",
    "Некоторые предлоги в русском языке могут управлять разными падежами (например, \"я еду в Лондон\" vs \"я живу в Лондоне\"). Давайте проанализируем эти предлоги и их падежи. Необходимо:\n",
    "\n",
    "- составить список таких предлогов (РГ-80 вам в помощь)\n",
    "- взять достаточно большой текст (можно большое художественное произведение)\n",
    "- сделать морфоразбор этого текста\n",
    "- Посчитать, как часто и какие падежи встречаются у слова, идущего после предлога.\n",
    "\n",
    "Примечания: во-первых, имейте в виду, что иногда после предлога могут идти самые неожиданные вещи: \"я что, должен ехать на, черт побери, северный полюс?\". Во-вторых, неплохо бы учитывать отсутствие пунктуации (конечно, в норме, как нам кажется, предлог обязательно требует зависимое, но! \"да иди ты на!\") Эти штуки можно отсеять, если просто учитывать только заранее определенные падежи, а не считать все, какие встретились (так и None можно огрести). \n",
    "\n",
    "Если будете использовать RNNMorph, возможно, понадобится регулярное выражение и немного терпения. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5586e858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предлог 'с':\n",
      "  ablt: 1385 раз(а)\n",
      "  gent: 383 раз(а)\n",
      "  accs: 89 раз(а)\n",
      "Предлог 'в':\n",
      "  loct: 1747 раз(а)\n",
      "  accs: 1042 раз(а)\n",
      "Предлог 'под':\n",
      "  accs: 54 раз(а)\n",
      "  ablt: 75 раз(а)\n",
      "Предлог 'на':\n",
      "  accs: 820 раз(а)\n",
      "  loct: 492 раз(а)\n",
      "Предлог 'о':\n",
      "  loct: 258 раз(а)\n",
      "  accs: 7 раз(а)\n",
      "Предлог 'по':\n",
      "  datv: 472 раз(а)\n",
      "  loct: 84 раз(а)\n",
      "  accs: 26 раз(а)\n",
      "Предлог 'за':\n",
      "  accs: 200 раз(а)\n",
      "  ablt: 175 раз(а)\n",
      "Предлог 'между':\n",
      "  ablt: 59 раз(а)\n"
     ]
    }
   ],
   "source": [
    "import pymorphy2\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "with open('тексты\\crime_and_punishment.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read() \n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "tokens = word_tokenize(text, language='russian')\n",
    "\n",
    "# Словарь предлогов и соответствующих падежей\n",
    "prepositions_cases = {\n",
    "    'в': ['accs', 'loct'], 'за': ['accs', 'ablt'], 'между': ['gent', 'ablt'],\n",
    "    'на': ['accs', 'loct'], 'о': ['accs', 'loct'], 'под': ['accs', 'ablt'],\n",
    "    'по': ['datv', 'accs', 'loct'], 'с': ['gent', 'accs', 'ablt']\n",
    "}\n",
    "\n",
    "# Словарь для подсчета падежей после предлогов\n",
    "case_count = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Анализ падежей слов после предлогов\n",
    "for i, word in enumerate(tokens):\n",
    "    if word.lower() in prepositions_cases:\n",
    "        next_word = tokens[i + 1]\n",
    "        parsed_word = morph.parse(next_word)[0]\n",
    "        case = parsed_word.tag.case\n",
    "        if case in prepositions_cases[word.lower()]:\n",
    "            case_count[word.lower()][case] += 1\n",
    "            \n",
    "for prep, cases in case_count.items():\n",
    "    print(f\"Предлог '{prep}':\")\n",
    "    for case, count in cases.items():\n",
    "        print(f'  {case}: {count} раз(а)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f8c885-9e8f-4bbc-a0ab-ac801af64245",
   "metadata": {},
   "source": [
    "#### Задача 3 (5 баллов). \n",
    "\n",
    "Представим, что у вас есть файл с разборами conllu (можете взять любой, например, [тут](https://github.com/dialogue-evaluation/GramEval2020)). Нужно просмотреть все примеры предложений с тегом dislocated и тегом discourse: напишите скрипт, который будет читать файл, находить все такие предложения и печатать: 1) сам текст предложения 2) слово, имеющее искомый тег. Если тег не был найден в файле, нужно об этом сообщить. Постарайтесь оформить вывод таким образом, чтобы это было удобно читать. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef478943-1842-4dfe-98dc-327860a907bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Короче : столько - то (discourse) \" я как Я \" , остальное \" я как МЫ \" , а если только первое , то это — привет психические расстройства .\n",
      "2. - Ой-ой-ой (discourse) , а сам - то (discourse) с мамой спишь каждый день , как маленький !\n",
      "3. - Ну (discourse) ты подожди ещё , у неё и дети будут .\n",
      "4. И они на ковре вертолете сваливают сначала на Ямайку , а потом в Бразилию , ну (discourse) это там где много диких обезьян в белых штанах .\n",
      "Следующие теги не были найдены: dislocated\n"
     ]
    }
   ],
   "source": [
    "def extract_sentences_with_tags(file_path):\n",
    "    sentences_with_tags = []\n",
    "    current_sentence_text = []\n",
    "    sentence_with_tag = False\n",
    "    tags_found = {'dislocated': False, 'discourse': False}\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if not line.isspace():\n",
    "                columns = line.split()\n",
    "                word, tag = columns[1], columns[7]\n",
    "                current_sentence_text.append(word)\n",
    "                if tag in tags_found:\n",
    "                    sentence_with_tag = True\n",
    "                    tags_found[tag] = True\n",
    "                    current_sentence_text[-1] = f\"{word} ({tag})\"\n",
    "            else:   # Конец предложения\n",
    "                if sentence_with_tag:\n",
    "                    sentences_with_tags.append(' '.join(current_sentence_text))\n",
    "\n",
    "                current_sentence_text = []\n",
    "                sentence_with_tag = False\n",
    "\n",
    "    tags_not_found = [tag for tag, found in tags_found.items() if not found]\n",
    "    return sentences_with_tags, tags_not_found\n",
    "\n",
    "\n",
    "sentences_with_tags, tags_not_found = extract_sentences_with_tags('GramEval2020-RuEval2017-social-dev.conllu')\n",
    "\n",
    "formatted_sentences = '\\n'.join(f'{i + 1}. {sentence}' for i, sentence in enumerate(sentences_with_tags))\n",
    "print(formatted_sentences)\n",
    "\n",
    "if tags_not_found:\n",
    "    print('Следующие теги не были найдены:', ', '.join(tags_not_found))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd0db77-e073-40ff-8c7d-3512325ce991",
   "metadata": {},
   "source": [
    "#### Задача 4 (5 баллов).\n",
    "\n",
    "Возьмите любой достаточно длинный (лучше новостной) текст. Любым известным инструментом извлеките именованные сущности из этого текста и выведите их списком по категориям (т.е. персоны вместе, локации вместе, организации вместе). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b0ff0b9-6bca-42a7-baea-f2c856e6059f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PER: Стэнфорда, Лоуренс Саммерс, Клодин Гэй, Адам Гийетт, Гийетта, Хиллари Клинтон, Йинона Коэна, Коэн, Надя Абу Эль-Хадж, Абу Эль-Хадж, Манан Ахмед, Манан Ахмед, Лиз Мэгилл, Клодин Гэй, Росс Стивенс\n",
      "LOC: Израиль, Газа, Нью-Йорка, Ближнем Востоке, Израиль, Газе, Палестиной, Израиль, Газе, Гарварда, Израиль, Нью-Йорке, США, Вашингтона, Израиль, Палестиной, Газа, США, Израилю, Палестине, Палестине, Палестины, Палестины, Южной Азии, Гарварда, Гарварда\n",
      "ORG: Администрации, The New Yorker, ХАМАС, Колумбийском университете, Медуза, The New Yorker, СМИ, ХАМАС, ЦАХАЛ, Комитет солидарности студентов, Гарвардского университета, Гарварда, ХАМАС, Колумбийском университете, ХАМАС, Университета Колумбия, Американском университете, Колумбийского университета, Стэнфордском университете, Колумбийского университета, The New Yorker, Колумбийского университета, Колумбийского университета, The New Yorker, The Guardian, Колумбийского университета, Колумбийского университета, Колумбийского университета, The New Yorker, ХАМАС, Конгрессе, Пенсильванского университета, Массачусетского технологического института (MIT), Пенсильванского университета, Конгрессе, Пенсильванского университета\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "with open(r'тексты\\news.txt', 'r', encoding='utf-8') as f:\n",
    "    news_text = f.read() \n",
    "\n",
    "nlp = spacy.load('ru_core_news_lg')\n",
    "doc = nlp(news_text)\n",
    "\n",
    "entities = {'PER': [], 'LOC': [], 'ORG': []}\n",
    "for ent in doc.ents:\n",
    "    entities[ent.label_].append(ent.text)\n",
    "\n",
    "for category, entity_list in entities.items():\n",
    "    print(f\"{category}: {', '.join(entity_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4762e8d-baca-4d22-b6f3-3866fee70a7d",
   "metadata": {},
   "source": [
    "#### Задача на бонусные 5 баллов:\n",
    "\n",
    "Сравните качество несколькиз разных морфопарсеров для любого языка, где их больше одного. Разберите этими морфопарсерами один и тот же текст, если они все разбирают в UD, можете вывести автоматически расхождения, в противном случае просмотрите глазами на наличие ошибок (текст, конечно, слишком большой лучше не брать). \n",
    "\n",
    "Без письменных выводов-комментариев не засчитывается. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89279821-5d4f-428e-b61c-33028a3d1374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
